import jax.numpy as jnp

import chex

from jax import jit

import haiku as hk

from gym import Env, spaces
from typing import Callable, Optional, Tuple

import base


class ClassificationEnv(Env):
  """Classification-based environment-based inference."""

  def __init__(self,
              logit_fn: Callable,
              x_train_generator: Callable,
              x_test_generator: Callable,
              prior_knowledge: base.PriorKnowledge,
              train_batch_size: int,
              test_batch_size:int,  
              num_steps:  int,
              key: chex.PRNGKey,
              override_train_data: Optional[Tuple[chex.Array, chex.Array]] = None):

    assert prior_knowledge.num_classes > 1

    # Key sequences
    rng = hk.PRNGSequence(key)
  
    '''logit_fn = make_mlp_logit_fn(
        input_dim=prior_knowledge.input_dim,
        temperature=prior_knowledge.temperature,
        hidden=prior_knowledge.hidden,
        num_classes=prior_knowledge.num_classes,
        key=next(rng),
    )'''

    self.logit_fn = logit_fn
    self.tau = prior_knowledge.tau
    self.x_test_generator = x_test_generator
    self.num_steps = num_steps

    input_dim = prior_knowledge.input_dim

    # Optionally override training data where you want to allow for training
    # data that was *not* generated by the x_generator, logit_fn.

    if override_train_data is None:
      nsamples = num_steps * train_batch_size
      (x_train, y_train), _ = base.sample_gaussian_data(
          logit_fn, x_train_generator,
          nsamples, next(rng))
      self.x_train = x_train.reshape((-1, train_batch_size, input_dim))
      self.y_train =  y_train.reshape((-1, train_batch_size, 1))

    else:
      x_train, y_train = override_train_data
      
      x_train = x_train.reshape((-1, train_batch_size, input_dim))
      y_train =  y_train.reshape((-1, train_batch_size, 1))

      assert train_batch_size == x_train.shape[1]
      assert train_batch_size == y_train.shape[1]
      self.train_data = (x_train, y_train)


    (x_test, y_test), _ = base.sample_gaussian_data(
            logit_fn, x_train_generator,
            nsamples, next(rng))
    self.x_test = x_test.reshape((-1, test_batch_size, input_dim))
    self.y_test =  y_test.reshape((-1, test_batch_size, 1))

    '''# Generate canonical x_test for DEBUGGING ONLY!!!
    num_test = 1000
    self.x_test = self.x_test_generator(next(rng), num_test)
    self.x_test = self.x_test.reshape((-1, train_batch_size, input_dim))

    test_logits = self.logit_fn(self.x_test)  # [n_train, n_class]
    #chex.assert_shape(test_logits.shape[1:], [num_test, prior_knowledge.num_classes])
    self.test_probs = nn.softmax(test_logits)'''
    
    # Environment OpenAI metadata
    self.reward_range = (0, 1)
    self.action_space = spaces.MultiDiscrete([prior_knowledge.num_classes] * train_batch_size)
    self.observation_space = {
                              "X_train":spaces.Box(low=-jnp.inf, high=jnp.inf, 
                                        shape=(train_batch_size, input_dim), dtype=jnp.float64),
                              "Y_train":spaces.Box(low=-jnp.inf, high=jnp.inf, 
                                        shape=(train_batch_size, 1), dtype=jnp.float64),
                              "X_test": spaces.Box(low=-jnp.inf, high=jnp.inf, 
                                        shape=(test_batch_size, input_dim), dtype=jnp.float64),
                              "Y_test": spaces.Box(low=-jnp.inf, high=jnp.inf, 
                                        shape=(test_batch_size, 1), dtype=jnp.float64)
                            }
    self.t = 0
    self.train_batch_size = train_batch_size
    self.test_batch_size = test_batch_size


  @property
  def done(self):
    return  self.t >= self.x_train.shape[0]

  def step(self, action):
    done = self.done
    info = {}

    y = self.y_train[self.t]
    reward = jnp.sum(y==action)
    self.t += 1
    
    if done:
      observation = {}
    else:
      observation = { "X_train": self.x_train[self.t],
                      "Y_train": self.y_train[self.t],
                      "X_test": self.x_test[self.t],
                      "Y_test": self.y_test[self.t]
                      }
    return observation, reward, done, info


  def test_data(self, key: chex.PRNGKey):
    """Generates test data and evaluates log likelihood w.r.t. environment.
    The test data that is output will be of length tau examples.
    We wanted to "pass" tau here... but ran into jit issues.
    Args:
      key: Random number generator key.
    Returns:
      Tuple of data (with tau examples) and log-likelihood under posterior.
    """
    def sample_test(k: chex.PRNGKey):
      return base.sample_gaussian_data(
          self.logit_fn, self.x_test_generator, self.tau, key=k)
    return jit(sample_test)(key)


  def render(self):
    pass